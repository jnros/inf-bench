# inf-bench

GPU microbenchmark: KV-cache attention (MHA, MQA, GQA) across sequence lengths.

## Usage

```
uv run main.py    # requires CUDA GPU, Python 3.11+, uv
```
